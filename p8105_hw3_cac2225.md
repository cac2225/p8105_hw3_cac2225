p8105\_hw3\_cac2225
================
Courtney Chan
October 8, 2018

``` r
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```

Homework 3
==========

Problem 1
---------

The dataset and library packages are first loaded.

``` r
library(p8105.datasets)
brfss_smart2010 = brfss_smart2010
```

``` r
library(tidyverse)
```

    ## -- Attaching packages --------------------------------------------- tidyverse 1.2.1 --

    ## v ggplot2 3.0.0     v purrr   0.2.5
    ## v tibble  1.4.2     v dplyr   0.7.6
    ## v tidyr   0.8.1     v stringr 1.3.1
    ## v readr   1.1.1     v forcats 0.3.0

    ## -- Conflicts ------------------------------------------------ tidyverse_conflicts() --
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(dplyr)
library(ggplot2)
library(patchwork)
library(forcats)
library(ggridges)
```

    ## 
    ## Attaching package: 'ggridges'

    ## The following object is masked from 'package:ggplot2':
    ## 
    ##     scale_discrete_manual

``` r
library(hexbin)
```

To clean the dataset, the variable names will be cleaned using the janitor package, function clean\_names. The data is filtered based on topic, Overall Health, and responses Excellent to Poor. The response variable will be turned into a factor variable and its responses ordered.

``` r
brfss_smart2010 = janitor::clean_names(brfss_smart2010)

filtered_brfss_smart2010 = filter(brfss_smart2010, topic == "Overall Health") %>% 
  filter(response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") %>% 
  mutate(response = factor(response, levels = str_c(c("Excellent", "Very good", "Good", "Fair", "Poor"))))
```

``` r
filtered_brfss_smart2010 %>% 
  filter(year == "2002") %>% 
  distinct(locationdesc) %>% 
  separate(locationdesc, into = c("state", "county"), sep = "-") %>%   
  count(state) %>% 
  filter(n == 7)
```

    ## Warning: Expected 2 pieces. Additional pieces discarded in 1 rows [25].

    ## # A tibble: 3 x 2
    ##   state     n
    ##   <chr> <int>
    ## 1 "CT "     7
    ## 2 "FL "     7
    ## 3 "NC "     7

To determine how many states were observed at 7 county locations, the number of distinct state-county values were determined, these values were seperated, and the number of times each state occurred was counted. The states that were counted as having appeared 7 times were filtered out. The states are as follows: CT, FL and NC.

``` r
filtered_brfss_smart2010 %>% 
  summarize(max_year = max(year), min_year = (min(year)))
```

    ## # A tibble: 1 x 2
    ##   max_year min_year
    ##      <dbl>    <dbl>
    ## 1     2010     2002

``` r
plot_spaghetti = filtered_brfss_smart2010 %>% 
  separate(locationdesc, into = c("state", "county"), sep = "-") %>% 
  group_by(year, state) %>% 
  summarize(n_counties = n())
```

    ## Warning: Expected 2 pieces. Additional pieces discarded in 40 rows [311,
    ## 312, 313, 314, 315, 1713, 1714, 1715, 1716, 1717, 3136, 3137, 3138, 3139,
    ## 3140, 4577, 4578, 4579, 4580, 4581, ...].

``` r
ggplot(plot_spaghetti, aes(x = year, y = n_counties, color = state)) + 
  geom_line() + 
  labs(
    title = "# of Locations per State, 2002 to 2010",
    y = "Number of Locations"
  ) + 
  theme(legend.background = element_rect(size = .5))
```

<img src="p8105_hw3_cac2225_files/figure-markdown_github/creating spaghetti plot number locations in each state, 2002 to 2010-1.png" width="90%" />

First I determined what the minimum and maximum year values are, which are 2002 and 2010, thus there was no need to filter by year any further. I created a new set of data, named plot\_spaghetti, seperating the variable locationdesc to ultimately count the number of times a state is recorded within a certain year. I plotted this setting year on the x axis, number of times a state has appeared which equals number of locations per state, and a different color for each state.

``` r
filtered_brfss_smart2010 %>% 
  filter(year %in% c("2002", "2006", "2010")) %>% 
  filter(response == "Excellent") %>% 
  filter(locationabbr == "NY") %>% 
  group_by(year, response, locationabbr) %>% 
  summarize(mean_Excellent = mean(data_value),
            sd_Excellent = sd(data_value)) %>% 
  knitr::kable(digits = 1)
```

|  year| response  | locationabbr |  mean\_Excellent|  sd\_Excellent|
|-----:|:----------|:-------------|----------------:|--------------:|
|  2002| Excellent | NY           |             24.0|            4.5|
|  2006| Excellent | NY           |             22.5|            4.0|
|  2010| Excellent | NY           |             22.7|            3.6|

Dataset was filtered by year (2002, 2006 and 2010), response (Excellent) and locationabbr (NY). The mean and standard deviation of the proportion of Excellent responses across NY state were calculated using the group by and summarize functions.

``` r
plot_avg_response = filtered_brfss_smart2010 %>% 
  group_by(year, locationabbr, response) %>% 
  summarize(proportion_response = mean(data_value))

plot_avg_response
```

    ## # A tibble: 2,215 x 4
    ## # Groups:   year, locationabbr [?]
    ##     year locationabbr response  proportion_response
    ##    <int> <chr>        <fct>                   <dbl>
    ##  1  2002 AK           Excellent                27.9
    ##  2  2002 AK           Very good                33.7
    ##  3  2002 AK           Good                     23.8
    ##  4  2002 AK           Fair                      8.6
    ##  5  2002 AK           Poor                      5.9
    ##  6  2002 AL           Excellent                18.5
    ##  7  2002 AL           Very good                30.9
    ##  8  2002 AL           Good                     32.7
    ##  9  2002 AL           Fair                     12.1
    ## 10  2002 AL           Poor                      5.9
    ## # ... with 2,205 more rows

``` r
ggplot(plot_avg_response, aes(x = year, y = proportion_response)) + 
  facet_grid(. ~ response) + 
  geom_violin() + 
  labs(
    title = "Five Panel Plot per Response type - distribution of State-level averages over time",
    y = "Proportion of each Response"
  ) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), text = element_text(size = 9)) + 
  theme(plot.title = element_text(size = 9))
```

    ## Warning: Removed 21 rows containing non-finite values (stat_ydensity).

<img src="p8105_hw3_cac2225_files/figure-markdown_github/avg proportion of each response per state-1.png" width="90%" />

The mean values of the proportion of each response was generated using group by and summarize functions, grouped by year and response type. Using facet, a five panel plot per response type was created, each panel contained a violin plot of proportion of responses over the years. Thus over the years, the majority of the proportion of excellent responses were mainly around 20. The max proportion was about 30 and min proportion was about 12 in 2006. Over the years, the majority of the proportion of very good responses were around 35, with max proportion at around 43 and min proportion at around 26 in 2006. The majority of the proportion of good responses were around 30, with max proportion at 36 and min at 28 in 2006. The majority of the proportion of fair responses were around 10, with max at 27, min at 7 in 2006. The majority of the proportion of poor responses were around 4, with min at 3, max at 9 in 2006.

Problem 2
---------

First the data Instacart is loaded and the package forcats is loaded to be used for later on in the problem.

``` rloading

instacart = instacart
```

Preliminary description of dataset
----------------------------------

``` r
names(instacart)
```

    ##  [1] "order_id"               "product_id"            
    ##  [3] "add_to_cart_order"      "reordered"             
    ##  [5] "user_id"                "eval_set"              
    ##  [7] "order_number"           "order_dow"             
    ##  [9] "order_hour_of_day"      "days_since_prior_order"
    ## [11] "product_name"           "aisle_id"              
    ## [13] "department_id"          "aisle"                 
    ## [15] "department"

``` r
instacart = instacart %>% 
  mutate(order_dow = recode(order_dow, `0` = "Sunday", `1` = "Monday", `2` = "Tuesday", `3` = "Wednesday", `4` = "Thursday", `5` = "Friday", `6` = "Saturday"))
```

To perform a preliminary review of the dataset, the names of the variables are first listed, by calling the names. The variable names all appear to be in lower snakecase. However, the names of the order\_dow values are recoded from numerals to actual names of the days of the week.

It seems that there are orders that are made with descriptions of the orders using variables such as giving each order an ID number (order\_id), listing the day (order\_dow) and hour (order\_hour\_of\_day) the order was made, days since the prior order was made (days\_since\_prior\_order) etc. Order\_dow has values from 0 to 6, each number corresponding to a day of the week, which have been recoded into actual days of the week. The orders consist of products which are assigned product ids (product\_id), described by what ailse and department they came from (aisle, aisle\_id, department, department\_id), ordered by which user (user\_id), and the product's name (product\_name). Key variables would include order\_id, user\_id, product\_id, department\_id and aisle\_id, since by having these variables, one can determine important information, such as what product was ordered by whom and where that product came from. The data structure appears tidy since the variables are listed as column names and corresponding values are listed under each variable.

To describe the size of the dataset, the number of observations and number of variables are 1384617, 15.

Answering questions about the dataset instacart
-----------------------------------------------

``` r
instacart %>% 
  count(n_distinct(aisle_id))
```

    ## # A tibble: 1 x 2
    ##   `n_distinct(aisle_id)`       n
    ##                    <int>   <int>
    ## 1                    134 1384617

``` r
instacart %>% 
    group_by(aisle, order_id) %>%  
    summarise(order_count = n()) %>% 
    group_by(aisle) %>% 
    summarise(total_order_count =
                sum(order_count)) %>%
  arrange(desc(total_order_count))
```

    ## # A tibble: 134 x 2
    ##    aisle                         total_order_count
    ##    <chr>                                     <int>
    ##  1 fresh vegetables                         150609
    ##  2 fresh fruits                             150473
    ##  3 packaged vegetables fruits                78493
    ##  4 yogurt                                    55240
    ##  5 packaged cheese                           41699
    ##  6 water seltzer sparkling water             36617
    ##  7 milk                                      32644
    ##  8 chips pretzels                            31269
    ##  9 soy lactosefree                           26240
    ## 10 bread                                     23635
    ## # ... with 124 more rows

Using the n\_distinct and count functions, there are 134 distinct aisle\_ids, thus there are 134 aisles. Using the group\_by and summarize functions, the number of orders made per aisle is counted and designated as order\_count. Then for each aisle, the number of orders made is summed and arranged in descending order. The top 3 aisles that received the most orders are "fresh vegetables" at 150609 orders, "fresh fruits" with 150473 orders and "packaged vegetable fruits" with 78493 orders.

``` r
orders_count = instacart %>% 
    group_by(aisle, aisle_id, department) %>%
    summarise(total_order_count = n()) %>% 
  arrange(desc(total_order_count))

ggplot(orders_count, aes(fct_reorder(aisle, desc(total_order_count)), total_order_count, fill = department)) + 
  geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), text = element_text(size = 9)) + 
  labs(
    title = "Most popular aisles - based on order count",
    y = "Number of orders") + 
      scale_x_discrete("Aisle", breaks = c("fresh vegetables", "bread", "cereal", "paper goods", "cookie cakes", "tortillas flat bread", "poultry counter", "prepared meals", "oral hygiene", "beers coolers", "hair care", "cocoa drink mixes", "bulk dried fruits vegetables", "beauty")) + 
  theme(legend.background = element_rect(size = .5))
```

<img src="p8105_hw3_cac2225_files/figure-markdown_github/number ordered items per aisle-1.png" width="90%" />

A subset of data is generated, counting the total number of items ordered from each aisle, using the group\_by and summarise functions. Using ggplot, the subset of data is plotted into a bar chart, with the aisles on the x axis and the total orders count on the y axis. The aisles are ordered from most to least orders, and with different colors representing different departments. The aisle that received the most orders is fresh vegetables with approximately 150,000 orders, and the aisle withthe least orders is beauty.

``` r
instacart %>% 
  filter(aisle %in% c("baking ingredients","dog food care","packaged vegetables fruits")) %>% 
    group_by(aisle, product_name) %>%
    summarise(product_count = n()) %>% 
  filter(product_count == max(product_count)) %>% 
   knitr::kable(digits = 1)
```

| aisle                      | product\_name                                 |  product\_count|
|:---------------------------|:----------------------------------------------|---------------:|
| baking ingredients         | Light Brown Sugar                             |             499|
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |              30|
| packaged vegetables fruits | Organic Baby Spinach                          |            9784|

To determine the most popular products in the three specific aisles, the dataset was first filterd by the three specific aisles. Then using the group\_by and summarise functions, the number of products bought from each aisle were counted up and the product with the largest count were extracted. The most popular item from the baking ingredients aisle is Light Brown Sugar with 499 units ordered. The most popular item from the dog food care aisle is snack sticks chicken and rice dog treats with 30 units ordered. The most popular item from the packaged vegetables and fruits aisle is organic baby spinach with 9784 units ordered.

``` r
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples","Coffee Ice Cream")) %>% 
  group_by(order_dow, product_name) %>%
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hour) %>% 
  knitr::kable(digits = 1)
```

| product\_name    |  Friday|  Monday|  Saturday|  Sunday|  Thursday|  Tuesday|  Wednesday|
|:-----------------|-------:|-------:|---------:|-------:|---------:|--------:|----------:|
| Coffee Ice Cream |    12.3|    14.3|      13.8|    13.8|      15.2|     15.4|       15.3|
| Pink Lady Apples |    12.8|    11.4|      11.9|    13.4|      11.6|     11.7|       14.2|

A 2x7 table is formed from the information filtered and summarized from the original dataset. Filter was used to extract information on the products pink lady apples and coffee ice cream. Group\_by and summarise were used to calculate the mean hour of the day that which these products are order, by day of the week. Spread and knit were used to form the final 2x7 table. Looking at the table, for Coffee Ice Cream, the earliest mean time of the day the product is ordered is on Fridays (day 5) at 12.3 The latest time it's ordered is on Tuesday (day 2) at 15.4. For Pink Lady Apples, the earliest this product is ordered is on average Mondays (day1) at 11.4 and latest on average on Sundays at 13.4.

Problem 3
=========

The dataset is first loaded and the variables are viewed. The variable names are all in lower snake case thus they are not modified.

``` r
ny_noaa = ny_noaa
```

The ny\_noaa dataset consists of seven variables that describe weather station (id), amount of precipitation (prcp), amount of snowfall and depth (snow and snwd), temperature (tmax and tmin) and date (date). The total number of observation rows and number of columns are 2595176, 7. Key variables would be id and date, since these provide specific identification for each observation. The dataset is tidy since each variable is its own seperate column, however the date column could be further seperated into month, year and day.

Each weather station might only collect a certain subset of variables, thus there is missing data. Missing data could cause an issue when we looking for trends, skewing or hiding any potential relationships that should normally appear in the data/plot visualizations.

``` r
ny_noaa = ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), sep = "-" ) %>% mutate(prcp = as.numeric(prcp)/10,
         tmin = as.numeric(tmin)/10,
         tmax = as.numeric(tmax)/10)
```

Since the values of prcp, tmin and tmax are originally in tenths of a millimeter, they should first be converted into numeric variables, then divided by 10 to convert into millimeters. Additionally, to seperate the date variable, the separate function is used and the values are split via the symbol "-".

``` r
ny_noaa %>% 
  group_by(snow) %>% 
  summarise(common_snowfall = n()) %>% 
  arrange(desc(common_snowfall))
```

    ## # A tibble: 282 x 2
    ##     snow common_snowfall
    ##    <int>           <int>
    ##  1     0         2008508
    ##  2    NA          381221
    ##  3    25           31022
    ##  4    13           23095
    ##  5    51           18274
    ##  6    76           10173
    ##  7     8            9962
    ##  8     5            9748
    ##  9    38            9197
    ## 10     3            8790
    ## # ... with 272 more rows

Using the group by and summarise functions, the most commonly observed values for snowfall are 0, NA and 25 mm. 0 may be the most common since it only really snows in the winter months in New York, which is three to four months out of the year. NA is very common since not all of the stations collect values on snowfall.

``` r
ny_noaa %>% 
  filter(month == "01" | month == "07") %>%
  group_by(id, month, year) %>% 
  summarise(mean_max_temp = mean(tmax)) %>% 
  ggplot(aes(x = id, y = mean_max_temp)) + 
  geom_point() + 
  facet_grid(. ~ month)
```

    ## Warning: Removed 7058 rows containing missing values (geom_point).

<img src="p8105_hw3_cac2225_files/figure-markdown_github/plot mean max temps January and July-1.png" width="90%" />

``` r
ny_noaa %>% 
  filter(month == "01" | month == "07") %>%
  filter(!is.na(tmax)) %>% 
  group_by(id, month, year) %>% 
  summarise(mean_max_temp = mean(tmax)) %>% 
  ggplot(aes(x = id, y = mean_max_temp)) + 
  geom_point() + 
  facet_grid(. ~ month)
```

<img src="p8105_hw3_cac2225_files/figure-markdown_github/plot mean max temps January and July-2.png" width="90%" />

Looking at the scatterplot produced, the cluster of points only occurs amongst a certain subset of stations. Thus confirming that only certain stations collect certain datapoints/information - in this case only certain stations collected termperature while the rest had missing data. Because there is a large number stations, I created a second graph to look at only the stations with non-missing data for max temperature.

Across all years, in January, the mean max temperatures are all in the lower range, varying from -10 to 10 degrees Celsius. There are a few outliers with temperatures lower than -10 degrees. Across all years, in July, the mean max temperatures are all in the higher range, varying from 20 to 30 degrees, with a few outliers below 20 degrees.

``` r
plot_temp = ny_noaa %>% ggplot(aes(x = tmin, y = tmax)) + geom_hex() + 
  labs(
    title = "Plot of t-min versus t max"
  )

plot_snow_v = ny_noaa %>% 
  filter(snow > 0, snow < 100) %>% 
  ggplot(aes(y = snow, x = year)) + 
  geom_violin() + 
  labs(
    title = "Distribution of Snowfall greater than 0 mm and less than 100 mm, for each year"
  ) + 
  theme(plot.title = element_text(size = 10)) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), text = element_text(size = 10))

(plot_temp)/(plot_snow_v)
```

    ## Warning: Removed 1136276 rows containing non-finite values (stat_binhex).

<img src="p8105_hw3_cac2225_files/figure-markdown_github/plot tmax vs t min-1.png" width="90%" />

I created a two panel plot with a hex graph of tmin versus tmax on top and violin graphs of snowfall distribution greater than 0 mm and less than 100 mm for each year on the bottom panel. The hexbin plot shows that as t min increases, so does t max, as expected, since in the warmer months, the minimum and maximum temperatures are expected to increase. While in the colder months, both tmax and tmin would decrease. For each year, the distribution of snowfall seems similar. The greatest number of temperature observations seems to have been recorded at 0 tmin and tmax, to 15 degrees tmin and 30 degrees tmax.

For the violin graph on snowfall, most of the snowfall received occurs between 0 to 25 mm. There are measurements of snowfall clustering around 50 and 75 as well, while the larger amounts of snowfall taper off at 100 mm.
