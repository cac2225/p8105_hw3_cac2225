p8105\_hw3\_cac2225
================
Courtney Chan
October 8, 2018

Homework 3
==========

Problem 1
---------

The dataset and library packages are first loaded.

``` r
library(p8105.datasets)
brfss_smart2010 = brfss_smart2010
```

``` r
library(tidyverse)
```

    ## -- Attaching packages ---------------------------------------------------------------- tidyverse 1.2.1 --

    ## v ggplot2 3.0.0     v purrr   0.2.5
    ## v tibble  1.4.2     v dplyr   0.7.6
    ## v tidyr   0.8.1     v stringr 1.3.1
    ## v readr   1.1.1     v forcats 0.3.0

    ## -- Conflicts ------------------------------------------------------------------- tidyverse_conflicts() --
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(dplyr)
library(ggplot2)
library(patchwork)
```

To clean the dataset, the variable names will be cleaned using the janitor package, function clean\_names. Change the variable names as needed. The data is filtered based on topic, Overall Health, and responses Excellent to Poor. The response variable will be turned into a factor variable and its responses ordered.

``` r
brfss_smart2010 = janitor::clean_names(brfss_smart2010)

filtered_brfss_smart2010 = filter(brfss_smart2010, topic == "Overall Health") %>% 
  filter(response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") %>% 
  mutate(response = factor(response, levels = str_c(c("Excellent", "Very good", "Good", "Fair", "Poor"))))
```

``` r
filtered_brfss_smart2010 %>% 
  filter(year == "2002") %>% 
  distinct(locationdesc) %>% 
  separate(locationdesc, into = c("state", "county"), sep = "-") %>%   
  count(state) %>% 
  filter(n == 7)
```

    ## Warning: Expected 2 pieces. Additional pieces discarded in 1 rows [25].

    ## # A tibble: 3 x 2
    ##   state     n
    ##   <chr> <int>
    ## 1 "CT "     7
    ## 2 "FL "     7
    ## 3 "NC "     7

To determine how many states were observed at 7 county locations, the number of distinct state-county values were determined, these values were seperated, and the number of times each state occurred was counted. The states that were counted as having appeared 7 times were filtered out. The states are as follows: CT, FL and NC.

``` r
filtered_brfss_smart2010 %>% 
  summarize(max_year = max(year), min_year = (min(year)))
```

    ## # A tibble: 1 x 2
    ##   max_year min_year
    ##      <dbl>    <dbl>
    ## 1     2010     2002

``` r
plot_spaghetti = filtered_brfss_smart2010 %>% 
  separate(locationdesc, into = c("state", "county"), sep = "-") %>% 
  group_by(year, state) %>% 
  summarize(n_counties = n())
```

    ## Warning: Expected 2 pieces. Additional pieces discarded in 40 rows [311,
    ## 312, 313, 314, 315, 1713, 1714, 1715, 1716, 1717, 3136, 3137, 3138, 3139,
    ## 3140, 4577, 4578, 4579, 4580, 4581, ...].

``` r
ggplot(plot_spaghetti, aes(x = year, y = n_counties, color = state)) + geom_line()
```

![](p8105_hw3_cac2225_files/figure-markdown_github/creating%20spaghetti%20plot%20number%20locations%20in%20each%20state,%202002%20to%202010-1.png)

First I determined what the minimum and maximum year values are, which are 2002 and 2010, thus there was no need to filter by year any further. I created a new set of data, named plot\_spaghetti, seperating the variable locationdesc to ultimately count the number of times a state is recorded within a certain year. I plotted this setting year on the x axis, number of times a state has appeared which equals number of locations per state, and a different color for each state.

``` r
table_filt_brss_smart2010 = filtered_brfss_smart2010 %>% 
  filter(year %in% c("2002", "2006", "2010")) %>% 
  filter(response == "Excellent") %>% 
  filter(locationabbr == "NY") %>% 
  group_by(year, response, locationabbr) %>% 
  summarize(mean_Excellent = mean(data_value),
            sd_Excellent = sd(data_value))
```

Dataset was filtered by year (2002, 2006 and 2010), response (Excellent) and locationabbr (NY). The mean and standard deviation of the proportion of Excellent responses across NY state were calculated using the group by and summarize functions.

``` r
plot_avg_response = filtered_brfss_smart2010 %>% 
  group_by(year, locationabbr, response) %>% 
  summarize(proportion_response = mean(data_value))

plot_excellent = plot_avg_response %>% 
  filter(response == "Excellent") %>% 
  ggplot(aes(x = year, y = proportion_response, color = locationabbr)) + 
  geom_line() + 
  theme(legend.position = "none") + 
  labs(y = "Proportion of Excellent Responses")

plot_very_good = plot_avg_response %>% 
  filter(response == "Very good") %>% 
  ggplot(aes(x = year, y = proportion_response, color = locationabbr)) + 
  geom_line() + 
  theme(legend.position = "none") + 
  labs(y = "Proportion of Very good Responses")
  
plot_good = plot_avg_response %>% 
  filter(response == "Good") %>% 
  ggplot(aes(x = year, y = proportion_response, color = locationabbr)) + 
  geom_line() + 
  theme(legend.position = "none") + 
  labs(y = "Proportion of Good Responses")
  
plot_fair = plot_avg_response %>% 
  filter(response == "Fair") %>% 
  ggplot(aes(x = year, y = proportion_response, color = locationabbr)) + 
  geom_line() + 
  theme(legend.position = "none") + 
  labs(y = "Proportion of Fair Responses")

plot_poor = plot_avg_response %>% 
  filter(response == "Poor") %>% 
  ggplot(aes(x = year, y = proportion_response, color = locationabbr)) + 
  geom_line() + 
  labs(y = "Proportion of Poor Responses"  + scale_color_hue(name = "State"))

(plot_excellent + plot_very_good + plot_good)/(plot_fair + plot_poor)
```

    ## Warning: Removed 3 rows containing missing values (geom_path).

    ## Warning: Removed 2 rows containing missing values (geom_path).

    ## Warning: Removed 2 rows containing missing values (geom_path).

    ## Warning: Removed 1 rows containing missing values (geom_path).

    ## Warning: Removed 1 rows containing missing values (geom_path).

![](p8105_hw3_cac2225_files/figure-markdown_github/avg%20proportion%20of%20each%20response%20per%20state-1.png)

The mean value of the proportion of each response was generated using group by and summarize functions, grouped by year and state and response type. Five different plots were generated based on this dataset, seperated by response type. Using patchwork, a five panel plot per response type was created, each panel contained a plot of proportion of responses over the years, and the distribution was shown for each state.

Problem 2
---------

First the data Instacart is loaded.

``` rloading

instacart = instacart
```

``` r
names(instacart)
```

    ##  [1] "order_id"               "product_id"            
    ##  [3] "add_to_cart_order"      "reordered"             
    ##  [5] "user_id"                "eval_set"              
    ##  [7] "order_number"           "order_dow"             
    ##  [9] "order_hour_of_day"      "days_since_prior_order"
    ## [11] "product_name"           "aisle_id"              
    ## [13] "department_id"          "aisle"                 
    ## [15] "department"

To perform a preliminary review of the dataset, the names of the variables are first listed, by calling the names.

It seems that there are orders that are made with descriptions of the orders using variables such as giving each order an ID number (order\_id), listing the day (order\_dow) and hour (order\_hour\_of\_day) the order was made, days since the prior order was made (days\_since\_prior\_order) etc. The orders consist of products which are assigned product ids (product\_id), described by what ailse and department they came from (aisle, aisle\_id, department, department\_id), ordered by which user (user\_id), and the product's name (product\_name). Key variables would include order\_id, user\_id, product\_id, department\_id and aisle\_id, since by having these variables, one can determine important information, such as what product was ordered by whom and where that product came from. The data structure appears tidy since the variables are listed as column names and corresponding values are listed under each variable.

To describe the size of the dataset, the number of observations and number of variables are 1384617, 15.

``` r
instacart %>% 
  count(n_distinct(aisle_id))
```

    ## # A tibble: 1 x 2
    ##   `n_distinct(aisle_id)`       n
    ##                    <int>   <int>
    ## 1                    134 1384617

``` r
instacart %>% 
    group_by(aisle, order_id) %>%  
    summarise(order_count = n()) %>% 
    group_by(aisle) %>% 
    summarise(total_order_count = sum(order_count)) %>% 
  arrange(desc(total_order_count))
```

    ## # A tibble: 134 x 2
    ##    aisle                         total_order_count
    ##    <chr>                                     <int>
    ##  1 fresh vegetables                         150609
    ##  2 fresh fruits                             150473
    ##  3 packaged vegetables fruits                78493
    ##  4 yogurt                                    55240
    ##  5 packaged cheese                           41699
    ##  6 water seltzer sparkling water             36617
    ##  7 milk                                      32644
    ##  8 chips pretzels                            31269
    ##  9 soy lactosefree                           26240
    ## 10 bread                                     23635
    ## # ... with 124 more rows

Using the n\_distinct and count functions, there are 134 distinct aisle\_ids, thus there are 134 aisles. Using the group\_by and summarize functions, the number of orders made per aisle is counted and designated as order\_count. Then for each aisle, the number of orders made is summed and arranged in descending order. The top 3 aisles that received the most orders are "fresh vegetables" at 150609 orders, "fresh fruits" with 150473 orders and "packaged vegetable fruits" with 78493 orders.
