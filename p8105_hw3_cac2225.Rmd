---
title: "p8105_hw3_cac2225"
author: "Courtney Chan"
date: "October 8, 2018"
output: github_document
---

```{r global settings for document}

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

```

#Homework 3

##Problem 1

The dataset and library packages are first loaded.

```{r loading BRSS data}

library(p8105.datasets)
brfss_smart2010 = brfss_smart2010

```

```{r upload necessary packages}

library(tidyverse)
library(dplyr)
library(ggplot2)
library(patchwork)

```

To clean the dataset, the variable names will be cleaned using the janitor package, function clean_names. Change the variable names as needed.
The data is filtered based on topic, Overall Health, and responses Excellent to Poor. The response variable will be turned into a factor variable and its responses ordered.

```{r data cleaning step}
brfss_smart2010 = janitor::clean_names(brfss_smart2010)

filtered_brfss_smart2010 = filter(brfss_smart2010, topic == "Overall Health") %>% 
  filter(response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") %>% 
  mutate(response = factor(response, levels = str_c(c("Excellent", "Very good", "Good", "Fair", "Poor"))))

```


```{r states observed at 7 locations in 2002}
filtered_brfss_smart2010 %>% 
  filter(year == "2002") %>% 
  distinct(locationdesc) %>% 
  separate(locationdesc, into = c("state", "county"), sep = "-") %>%   
  count(state) %>% 
  filter(n == 7)
```

To determine how many states were observed at 7 county locations, the number of distinct state-county values were determined, these values were seperated, and the number of times each state occurred was counted. The states that were counted as having appeared 7 times were filtered out.
The states are as follows: CT, FL and NC.

```{r creating spaghetti plot number locations in each state, 2002 to 2010}
filtered_brfss_smart2010 %>% 
  summarize(max_year = max(year), min_year = (min(year)))

plot_spaghetti = filtered_brfss_smart2010 %>% 
  separate(locationdesc, into = c("state", "county"), sep = "-") %>% 
  group_by(year, state) %>% 
  summarize(n_counties = n())

ggplot(plot_spaghetti, aes(x = year, y = n_counties, color = state)) + geom_line()
```

First I determined what the minimum and maximum year values are, which are 2002 and 2010, thus there was no need to filter by year any further. I created a new set of data, named plot_spaghetti, seperating the variable locationdesc to ultimately count the number of times a state is recorded within a certain year. I plotted this setting year on the x axis, number of times a state has appeared which equals number of locations per state, and a different color for each state.

```{r table of 2002, 2006 and 2010 proportion of Excellent responses in NY}
table_filt_brss_smart2010 = filtered_brfss_smart2010 %>% 
  filter(year %in% c("2002", "2006", "2010")) %>% 
  filter(response == "Excellent") %>% 
  filter(locationabbr == "NY") %>% 
  group_by(year, response, locationabbr) %>% 
  summarize(mean_Excellent = mean(data_value),
            sd_Excellent = sd(data_value))
```  

Dataset was filtered by year (2002, 2006 and 2010), response (Excellent) and locationabbr (NY). The mean and standard deviation of the proportion of Excellent responses across NY state were calculated using the group by and summarize functions.

```{r avg proportion of each response per state}

plot_avg_response = filtered_brfss_smart2010 %>% 
  group_by(year, locationabbr, response) %>% 
  summarize(proportion_response = mean(data_value))

plot_excellent = plot_avg_response %>% 
  filter(response == "Excellent") %>% 
  ggplot(aes(x = year, y = proportion_response, color = locationabbr)) + 
  geom_line() + 
  theme(legend.position = "none") + 
  labs(y = "Excellent Responses")

plot_very_good = plot_avg_response %>% 
  filter(response == "Very good") %>% 
  ggplot(aes(x = year, y = proportion_response, color = locationabbr)) + 
  geom_line() + 
  theme(legend.position = "none") + 
  labs(y = "Very good Responses")
  
plot_good = plot_avg_response %>% 
  filter(response == "Good") %>% 
  ggplot(aes(x = year, y = proportion_response, color = locationabbr)) + 
  geom_line() + 
  theme(legend.position = "right") + 
  labs(y = "Good Responses")
  
plot_fair = plot_avg_response %>% 
  filter(response == "Fair") %>% 
  ggplot(aes(x = year, y = proportion_response, color = locationabbr)) + 
  geom_line() + 
  theme(legend.position = "none") + 
  labs(y = "Fair Responses")

plot_poor = plot_avg_response %>% 
  filter(response == "Poor") %>% 
  ggplot(aes(x = year, y = proportion_response, color = locationabbr)) + 
  geom_line() + 
  theme(legend.position = "none") + 
  labs(y = "Poor Responses")

(plot_excellent)/(plot_very_good)/(plot_good)/(plot_fair)/(plot_poor)

```

The mean value of the proportion of each response was generated using group by and summarize functions, grouped by year and state and response type. Five different plots were generated based on this dataset, seperated by response type. Using patchwork, a five panel plot per response type was created, each panel contained a plot of proportion of responses over the years, and the distribution was shown for each state.

##Problem 2

First the data Instacart is loaded and the package forcats is loaded to be used for later on in the problem.

```{rloading Instacart data}

library(forcats)

instacart = instacart

```

```{r Preliminary review of the data}

names(instacart)

```

To perform a preliminary review of the dataset, the names of the variables are first listed, by calling the names.

It seems that there are orders that are made with descriptions of the orders using variables such as giving each order an ID number (order_id), listing the day (order_dow) and hour (order_hour_of_day) the order was made, days since the prior order was made (days_since_prior_order) etc. Order_dow has values from 0 to 6, each number corresponding to a day of the week. The orders consist of products which are assigned product ids (product_id), described by what ailse and department they came from (aisle, aisle_id, department, department_id), ordered by which user (user_id), and the product's name (product_name). Key variables would include order_id, user_id, product_id, department_id and aisle_id, since by having these variables, one can determine important information, such as what product was ordered by whom and where that product came from. The data structure appears tidy since the variables are listed as column names and corresponding values are listed under each variable.

To describe the size of the dataset, the number of observations and number of variables are `r dim(instacart)`.

```{r # of aisles and aisles with most orders}

instacart %>% 
  count(n_distinct(aisle_id))

instacart %>% 
    group_by(aisle, order_id) %>%  
    summarise(order_count = n()) %>% 
    group_by(aisle) %>% 
    summarise(total_order_count =
                sum(order_count)) %>%
  arrange(desc(total_order_count))

```

Using the n_distinct and count functions, there are 134 distinct aisle_ids, thus there are 134 aisles. Using the group_by and summarize functions, the number of orders made per aisle is counted and designated as order_count. Then for each aisle, the number of orders made is summed and arranged in descending order. The top 3 aisles that received the most orders are "fresh vegetables" at 150609 orders, "fresh fruits" with 150473 orders and "packaged vegetable fruits" with 78493 orders.

```{r number ordered items per aisle}

orders_count = instacart %>% 
    group_by(aisle, order_id, department, department_id) %>%
    summarise(order_count = n()) %>% 
    group_by(aisle, department, department_id) %>% 
    summarise(total_order_count =
                sum(order_count))

ggplot(orders_count, aes(fct_reorder(aisle,  department_id), total_order_count, fill = department_id)) + 
  geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) 

```

A subset of data is generated, counting the total number of items ordered from each aisle, using the group_by and summarise functions. Using ggplot, the subset of data is plotted into a bar chart, with the aisles on the x axis and the total orders count on the y axis. The aisles are ordered and grouped by department id, with different colors representing different departments.

```{r most popular item in certain aisles}

most_popular_products = instacart %>% 
  filter(aisle %in% c("baking ingredients","dog food care","packaged vegetables fruits")) %>% 
    group_by(aisle, product_name) %>%
    summarise(product_count = n()) %>% 
  filter(product_count == max(product_count))

```

To determine the most popular products in the three specific aisles, the dataset was first filterd by the three specific aisles. Then using the group_by and summarise functions, the number of products bought from each aisle were counted up and the product with the largest count were extracted. The most popular item from the baking ingredients aisle is Light Brown Sugar with 499 units ordered. The most popular item from the dog food care aisle is snack sticks chicken and rice dog treats with 30 units ordered. The most popular item from the packaged vegetables and fruits aisle is organic baby spinach with 9784 units ordered.

```{r Pink lady apples and coffee ice cream}

instacart %>% 
  filter(product_name %in% c("Pink Lady Apples","Coffee Ice Cream")) %>% 
  group_by(order_dow, product_name) %>%
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  spread(key = order_dow, value = mean_hour) %>% 
  knitr::kable(digits = 1)

```

A 2x7 table is formed from the information filtered and summarized from the original dataset. Filter was used to extract information on the products pink lady apples and coffee ice cream. Group_by and summarise were used to calculate the mean hour of the day that which these products are order, by day of the week. Spread and knit were used to form the final 2x7 table. Looking at the table, for Coffee Ice Cream, the earliest mean time of the day the product is ordered is on Fridays (day 5) at 12.3 The latest time it's ordered is on Tuesday (day 2) at 15.4. For Pink Lady Apples, the earliest this product is ordered is on average Mondays (day1) at 11.4 and latest on average on Sundays at 13.4.

#Problem 3